{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3f59ace",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "753a0d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5846d368",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_rows', 999)\n",
    "pd.set_option('display.max_columns', 999)\n",
    "\n",
    "pd.options.display.float_format = '{:,.4f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191f95f4",
   "metadata": {},
   "source": [
    "# Path Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6e478e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the current working directory instead\n",
    "base_path = os.getcwd()\n",
    "data_source_dir = os.path.abspath(os.path.join(base_path, \"..\", \"..\", \"A. Data Source\", \"A.2. SSKI (Bank Indonesia)\"))\n",
    "data_result_dir = os.path.abspath(os.path.join(base_path, \"..\", \"..\", \"C. Processed Data\", \"C.2. SSKI (Bank Indonesia)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a983a2c",
   "metadata": {},
   "source": [
    "# Additional Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abc0c1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sheet_15a_or_16a(file_path):\n",
    "    try:\n",
    "        # Get all sheet names\n",
    "        sheet_names = pd.ExcelFile(file_path).sheet_names\n",
    "\n",
    "        # Select the appropriate sheet\n",
    "        target_sheet = None\n",
    "        if '15a' in sheet_names:\n",
    "            target_sheet = '15a'\n",
    "        elif '16a' in sheet_names:\n",
    "            target_sheet = '16a'\n",
    "\n",
    "        if target_sheet:\n",
    "            df = pd.read_excel(file_path, sheet_name=target_sheet)\n",
    "            return df\n",
    "        else:\n",
    "            print(f\"No sheet '15a' or '16a' found in {file_path}\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "268753a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    \"\"\"Convert text to lowercase and remove all spaces.\"\"\"\n",
    "    return str(text).lower().replace(\" \", \"\")\n",
    "\n",
    "def merge_rows(df, merge_list, col_index=1):\n",
    "    # Normalize the merge_list\n",
    "    normalized_list = [normalize_text(item) for item in merge_list]\n",
    "\n",
    "    rows_to_drop = []\n",
    "    for i in range(len(df) - 1):\n",
    "        current_val = normalize_text(df.iloc[i, col_index])\n",
    "\n",
    "        if current_val in normalized_list:\n",
    "            # Merge current row with the next row (column by column)\n",
    "            for col in df.columns:\n",
    "                val1 = df.at[i, col]\n",
    "                val2 = df.at[i + 1, col]\n",
    "\n",
    "                # Convert nulls to empty string, others to string\n",
    "                str1 = \"\" if pd.isna(val1) else str(val1)\n",
    "                str2 = \"\" if pd.isna(val2) else str(val2)\n",
    "\n",
    "                # Merge with a space only if both are non-empty\n",
    "                if str1 and str2:\n",
    "                    merged = str1 + \" \" + str2\n",
    "                else:\n",
    "                    merged = str1 + str2  # One of them is empty\n",
    "\n",
    "                df.at[i, col] = merged\n",
    "\n",
    "            rows_to_drop.append(i + 1)\n",
    "\n",
    "    df = df.drop(rows_to_drop).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc33ce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_dataframe_target(df:pd.DataFrame, col_index:int, target_value:str)-> pd.DataFrame :\n",
    "    col_name = df.columns[col_index]\n",
    "\n",
    "    # print(f\"Columns name to searched and cut : {col_name}\")\n",
    "    standardized_target = target_value.lower().replace(' ', '')\n",
    "\n",
    "    # Standardize column 1 values\n",
    "    standardized_col = df[col_name].str.lower().str.replace(' ', '', regex=False)\n",
    "\n",
    "    # Find first match\n",
    "    start_index = df[standardized_col == standardized_target].index.min()\n",
    "                                                                                                           \n",
    "    # Filter rows from that index down\n",
    "    if(start_index >= 3) :\n",
    "        filtered_df = df.loc[start_index:]\n",
    "    else :\n",
    "        filtered_df = df.loc[0:]\n",
    "\n",
    "    return(filtered_df.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1189ddcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_q_strings_inplace(df: pd.DataFrame):\n",
    "    first_row = df.iloc[0].tolist()  # Make a copy to iterate safely\n",
    "\n",
    "    for idx, val in enumerate(first_row):\n",
    "        if isinstance(val, str) and val.startswith('Q'):\n",
    "            # Search leftward\n",
    "            for left_idx in range(idx - 1, -1, -1):\n",
    "                left_val = str(first_row[left_idx])\n",
    "                if re.match(r'^\\d', left_val):  # Starts with digit\n",
    "                    digits = re.findall(r'\\d', left_val)\n",
    "                    if len(digits) >= 4:\n",
    "                        left_number = ''.join(digits[:4])\n",
    "                        new_val = f\"{left_number} {val}\"\n",
    "                        df.iat[0, idx] = new_val  # Update value in-place\n",
    "                    break  # Stop after first match\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "227aff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe_and_trim_nulls(df):\n",
    "    # Step 1: Remove first and last two columns\n",
    "    df = df.iloc[:, 1:-2]\n",
    "\n",
    "    # Step 2: Reset column index\n",
    "    df.columns = range(df.shape[1])\n",
    "\n",
    "    # Step 3: Set first cell to \"KOMPONEN\"\n",
    "    df.iat[0, 0] = \"KOMPONEN\"\n",
    "\n",
    "    # Step 4: Always remove last row if:\n",
    "    # - The entire row is null, or\n",
    "    # - The value in first column is null\n",
    "    while df.shape[0] > 0:\n",
    "        last_idx = df.index[-1]\n",
    "        if df.iloc[last_idx].isnull().all() or pd.isna(df.iat[last_idx, 0]):\n",
    "            df = df.drop(index=last_idx)\n",
    "        else:\n",
    "            break  # Stop when a valid last row is found\n",
    "\n",
    "    # Step 5: Reset index\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1739e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_section_headers(df:pd.DataFrame, search_values:list, new_col_name:str, target_col_index:int=0):\n",
    "    # Step 1: Insert new column at the beginning\n",
    "    df.insert(0, new_col_name, None)\n",
    "\n",
    "    # Step 2: Normalize search values once\n",
    "    normalized_search = [normalize_text(val) for val in search_values]\n",
    "\n",
    "    # Step 3: Convert column to list for processing\n",
    "    col_values = df.iloc[:, target_col_index + 1].tolist()  # +1 because of inserted column\n",
    "\n",
    "    # Step 4: Track index manually because we'll be deleting rows\n",
    "    i = 0\n",
    "    while i < len(col_values):\n",
    "        val = col_values[i]\n",
    "        norm_val = normalize_text(val)\n",
    "\n",
    "        if norm_val in normalized_search:\n",
    "            header = val  # Keep original text for tagging\n",
    "            start_idx = i\n",
    "\n",
    "            # Remove the header row\n",
    "            df = df.drop(index=start_idx)\n",
    "            df = df.reset_index(drop=True)\n",
    "\n",
    "            # Rebuild col_values after drop\n",
    "            col_values = df.iloc[:, target_col_index + 1].tolist()\n",
    "\n",
    "            # Fill new_col_name from start_idx until next header or end\n",
    "            end_idx = start_idx\n",
    "            while end_idx < len(col_values) and normalize_text(col_values[end_idx]) not in normalized_search:\n",
    "                df.at[end_idx, new_col_name] = header\n",
    "                end_idx += 1\n",
    "\n",
    "            # Start next search at end_idx\n",
    "            i = end_idx\n",
    "            col_values = df.iloc[:, target_col_index + 1].tolist()\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d643c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_tag_section_by_null_above(df: pd.DataFrame, new_col_name: str, target_col_index: int = 1):\n",
    "    # Step 1: Insert the new column at position 1 (second column)\n",
    "    df.insert(1, new_col_name, None)\n",
    "\n",
    "    # Step 2: Prepare tracking\n",
    "    i = 1  # Start from second row since we'll compare with row - 1\n",
    "    current_tag = None\n",
    "\n",
    "    while i < len(df):\n",
    "        current_val = df.iat[i, target_col_index + 1]  # +1 because of inserted column\n",
    "        prev_val = df.iat[i - 1, target_col_index + 1]\n",
    "\n",
    "        if pd.notna(current_val) and pd.isna(prev_val):\n",
    "            # New section detected\n",
    "\n",
    "            # Store the tag (original text)\n",
    "            current_tag = current_val\n",
    "\n",
    "            # Remove both tag row and null row above it\n",
    "            df = df.drop(index=[i - 1, i])\n",
    "            df = df.reset_index(drop=True)\n",
    "\n",
    "            # Recalculate total length\n",
    "            i -= 1  # Step back to safely continue from the right index\n",
    "            if i < 0:\n",
    "                i = 0\n",
    "        else:\n",
    "            if current_tag:\n",
    "                df.iat[i, 1] = current_tag  # Set the tag into the new column\n",
    "            i += 1\n",
    "\n",
    "    # Fill the last part of the dataframe with the final tag\n",
    "    for j in range(i, len(df)):\n",
    "        if current_tag:\n",
    "            df.iat[j, 1] = current_tag\n",
    "\n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0706dcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_tag_section_by_null_right(df: pd.DataFrame, new_col_name: str, target_col_index: int = 2):\n",
    "    # Step 1: Insert new column at position 2 (third column)\n",
    "    df.insert(target_col_index, new_col_name, None)\n",
    "\n",
    "    current_tag = None\n",
    "    i = 0\n",
    "\n",
    "    while i < len(df):\n",
    "        current_val = df.iat[i, target_col_index + 1]      # Value in the tag column (newly inserted)\n",
    "        right_val = df.iat[i, target_col_index + 2]     # Value in the original \"right\" column (index 3 originally)\n",
    "\n",
    "        if pd.notna(current_val) and pd.isna(right_val):\n",
    "            # Treat this row as a tag\n",
    "            current_tag = current_val\n",
    "\n",
    "            # Remove the tag row\n",
    "            df = df.drop(index=i)\n",
    "            df = df.reset_index(drop=True)\n",
    "\n",
    "            # No increment â€” stay on the same row after drop\n",
    "            continue\n",
    "        else:\n",
    "            if current_tag:\n",
    "                df.iat[i, target_col_index] = current_tag\n",
    "        i += 1\n",
    "\n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5883d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditionally_clear_ket_data_level_3_per_row(df: pd.DataFrame, check_str : str) -> pd.DataFrame:\n",
    "    if 'ket_data_level_3' not in df.columns:\n",
    "        return df  # Do nothing if the column doesn't exist\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        val = df.iat[i, 0]  # Value in column index 0\n",
    "        if normalize_text(val) != normalize_text(check_str):\n",
    "            df.at[i, 'ket_data_level_3'] = None  # or use np.nan\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55225302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_year_strings(lst:list) :\n",
    "    processed = []\n",
    "    for text in lst:\n",
    "        match = re.match(r'^(\\d{4})(\\S)(.*)', text)\n",
    "        if match:\n",
    "            # Find first space after the 4-digit year\n",
    "            rest = text[4:]\n",
    "            if rest and not rest.startswith(\" \"):\n",
    "                first_space_idx = rest.find(\" \")\n",
    "                if first_space_idx != -1:\n",
    "                    new_text = text[:4] + rest[first_space_idx:]\n",
    "                else:\n",
    "                    new_text = text[:4]  # No space found after year\n",
    "                processed.append(new_text.strip())\n",
    "            else:\n",
    "                processed.append(text)  # Already has space\n",
    "        else:\n",
    "            processed.append(text)  # Doesn't start with 4 digits\n",
    "    return processed\n",
    "\n",
    "def rename_columns_from_first_row(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Step 1: Get the current column names\n",
    "    current_cols = df.columns.tolist()\n",
    "\n",
    "    # Step 2: Get the first row as new names (for columns index 3 onward)\n",
    "    new_names = df.iloc[0, 3:].tolist()\n",
    "\n",
    "    # Clean string for column naming\n",
    "    new_names = clean_year_strings(new_names)\n",
    "\n",
    "    # Step 3: Combine fixed names (first 3) + new names from first row\n",
    "    updated_cols = current_cols[:3] + new_names\n",
    "\n",
    "    # Step 4: Assign new column names\n",
    "    df.columns = updated_cols\n",
    "\n",
    "    # Step 5: Drop the first row\n",
    "    df = df.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7d12424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_na_extension(df : pd.DataFrame) -> pd.DataFrame :\n",
    "    # Fill column ket_data_level_1 and ket_data_level_2\n",
    "    df['ket_data_level_1'] = df['ket_data_level_1'].fillna('INTRO')\n",
    "    df['ket_data_level_2'] = df['ket_data_level_2'].fillna('INTRO')\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        val = df.iat[i, 0]  # Value in column index 0\n",
    "        if normalize_text(val) == normalize_text('INTRO'):\n",
    "            df.at[i, 'ket_data_level_3'] = 'Intro'  # or use np.nan\n",
    "        elif normalize_text(val) == normalize_text('DATA KINERJA KEUANGAN'):\n",
    "            df.at[i, 'ket_data_level_3'] = 'Data Kinerja Keuangan'  # or use np.nan\n",
    "        elif normalize_text(val) == normalize_text('INDIKATOR HASIL SURVEI 3)'):\n",
    "            df.at[i, 'ket_data_level_3'] = 'Indikator Hasil Survei'  # or use np.nan\n",
    "    \n",
    "    # Add feature index for ordering based on feature\n",
    "    df = df.reset_index()\n",
    "    df = df.rename(columns={'index': 'feature_index'})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "584c6ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_df_long(df:pd.DataFrame):\n",
    "    # Identify year-based columns\n",
    "    year_cols = [col for col in df.columns if re.match(r'^\\d{4}', str(col))]\n",
    "    other_cols = [col for col in df.columns if col not in year_cols]\n",
    "\n",
    "    # Melt: POSISI and KOMPONEN both as rows\n",
    "    df_long = df.melt(\n",
    "        id_vars= other_cols,\n",
    "        value_vars=year_cols,\n",
    "        var_name='POSISI',\n",
    "        value_name='VALUE'\n",
    "    )\n",
    "\n",
    "    return df_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aec3900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def end_to_end_df_processing(df:pd.DataFrame) -> pd.DataFrame :\n",
    "    merge_key_list = [\n",
    "        \"KOMPONEN\"\n",
    "    ]\n",
    "    \n",
    "    target_value = \"KOMPONEN\"\n",
    "\n",
    "    value_list = [\n",
    "        \"RASIO KINERJA KEUANGAN\",\n",
    "        \"DATA KINERJA KEUANGAN\",\n",
    "        \"INDIKATOR HASIL SURVEI 3)\"\n",
    "    ]\n",
    "\n",
    "    process_df_0 = merge_rows(df, merge_key_list, 0)\n",
    "    process_df_1 = cut_dataframe_target(process_df_0.copy(), 0, target_value)\n",
    "    process_df_2 = process_q_strings_inplace(process_df_1.copy())\n",
    "    process_df_3 = clean_dataframe_and_trim_nulls(process_df_2.copy())\n",
    "    process_df_4 = tag_section_headers(process_df_3.copy(), value_list, \"ket_data_level_1\")\n",
    "    process_df_5 = auto_tag_section_by_null_above(process_df_4.copy(), \"ket_data_level_2\")\n",
    "    process_df_6 = auto_tag_section_by_null_right(process_df_5.copy(), \"ket_data_level_3\")\n",
    "    process_df_7 = conditionally_clear_ket_data_level_3_per_row(process_df_6.copy(), \"RASIO KINERJA KEUANGAN\")\n",
    "    process_df_8 = rename_columns_from_first_row(process_df_7.copy())\n",
    "    process_df_9 = fill_na_extension(process_df_8.copy())\n",
    "    process_df_10 = transform_df_long(process_df_9.copy())\n",
    "\n",
    "    return(process_df_10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7948082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_two_dfs(df_list : list) -> pd.DataFrame :\n",
    "    key_cols = ['ket_data_level_1', 'ket_data_level_2', 'ket_data_level_3', 'KOMPONEN', 'POSISI']\n",
    "\n",
    "    combined = pd.concat(df_list, ignore_index=True)\n",
    "    combined = combined.drop_duplicates(subset=key_cols, keep='last')\n",
    "    return(combined.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018c5807",
   "metadata": {},
   "source": [
    "# Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de961555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found excel file : SSKI_DESEMBER_2022.xlsx\n",
      "Found excel file : SSKI_DESEMBER_2023.xlsx\n",
      "Found excel file : SSKI_DESEMBER_2024.xlsx\n",
      "Found excel file : SSKI_JUNI 2025.xlsx\n",
      "Found excel file : SSKI_JUNI_2022.xlsx\n",
      "Found excel file : SSKI_JUNI_2023.xlsx\n",
      "Found excel file : SSKI_JUNI_2024.xlsx\n"
     ]
    }
   ],
   "source": [
    "data_dict = {}\n",
    "\n",
    "# Loop through all child folders\n",
    "for folder in os.listdir(data_source_dir):\n",
    "    folder_path = os.path.join(data_source_dir, folder)\n",
    "    \n",
    "    # Only process if it's a directory\n",
    "    if os.path.isdir(folder_path):\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.endswith('.xlsx'):\n",
    "                file_path = os.path.join(folder_path, file)\n",
    "                try:\n",
    "                    print(f\"Found excel file : {file}\")\n",
    "                    df = read_sheet_15a_or_16a(file_path)\n",
    "                    key = os.path.splitext(file)[0]  # Get filename without extension\n",
    "                    data_dict[key] = df\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to read {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1a03bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_backup = data_dict.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "127e55fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['SSKI_DESEMBER_2022', 'SSKI_DESEMBER_2023', 'SSKI_DESEMBER_2024', 'SSKI_JUNI 2025', 'SSKI_JUNI_2022', 'SSKI_JUNI_2023', 'SSKI_JUNI_2024'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed5a6e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = data_dict_backup.copy()\n",
    "combined_all_df = pd.DataFrame()\n",
    "key_list_ordered = [\n",
    "    'SSKI_JUNI_2022', 'SSKI_DESEMBER_2022', \n",
    "    'SSKI_JUNI_2023', 'SSKI_DESEMBER_2023', \n",
    "    'SSKI_JUNI_2024', 'SSKI_DESEMBER_2024',\n",
    "    'SSKI_JUNI_2025'\n",
    "]\n",
    "\n",
    "for key in data_dict.keys() :\n",
    "    df = data_dict[key]\n",
    "    new_df = end_to_end_df_processing(df)\n",
    "    data_dict[key] = new_df\n",
    "    combined_all_df = combine_two_dfs([combined_all_df, new_df])\n",
    "\n",
    "combined_all_df = combined_all_df.sort_values(by=['POSISI', 'feature_index'], ascending=[True, True]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d22692b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_order = [\n",
    "    'POSISI', 'POSISI_TYPE', 'feature_index', 'ket_data_level_1', \n",
    "    'ket_data_level_2', 'ket_data_level_3', 'KOMPONEN', 'SATUAN/ UNIT', \n",
    "    'VALUE'\n",
    "]\n",
    "\n",
    "combined_all_df['POSISI_TYPE'] = np.where(combined_all_df['POSISI'].str.endswith('**'), 'Angka Sangat Sementara',\n",
    "                     np.where(combined_all_df['POSISI'].str.endswith(' *'), 'Angka Sementara',\n",
    "                              'Angka Tetap'))\n",
    "combined_all_df = combined_all_df.sort_values(by=['POSISI', 'feature_index'], ascending=[True, True]).reset_index(drop=True)\n",
    "\n",
    "combined_all_df = combined_all_df[new_column_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b392c68b",
   "metadata": {},
   "source": [
    "# Export Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5ad54d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file_name = \"SSKI_16a_result.xlsx\"\n",
    "\n",
    "result_file_path = os.path.join(data_result_dir, result_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "050d96b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_all_df.to_excel(result_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a81b169",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.read_excel(result_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b01f103",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = result_df.rename(\n",
    "    columns={\n",
    "        \"feature_index\": \"FEATURE_INDEX\",\n",
    "        \"ket_data_level_1\":\"TIPE_DATA\",\n",
    "        \"ket_data_level_2\":\"SEKTOR_EKONOMI\",\n",
    "        \"ket_data_level_3\":\"TIPE_KOMPONEN\",\n",
    "        \"SATUAN/ UNIT\":\"SATUAN_UNIT\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Replace only exact \"-\"\n",
    "result_df[\"VALUE\"] = result_df[\"VALUE\"].replace(\"-\", np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "409e5ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_excel(result_file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
